{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU-accelerated Data Science Workflow\n",
    "\n",
    "## Human Activity Recognition using GPU DataFrame and GPU KMeans\n",
    "\n",
    "Analyzing smart phone sensors to determine the activity the person is engaged in.\n",
    "\n",
    "\n",
    "- Activities: biking, sitting, standing, walking, stairup, stairdown.\n",
    "- Sensors: Accelerometer and Gyroscope.\n",
    "- Sampling rate: highest frequency the respective device allows.\n",
    "\n",
    "Link to the dataset: http://archive.ics.uci.edu/ml/datasets/Heterogeneity+Activity+Recognition\n",
    "\n",
    "\n",
    "Our approach uses KMeans from the `h2o4gpu` package to form the initial clusters.  Then, we use nearest neighbour to classify the clusters; i.e. the intra-cluster dominating class determines the class for the cluster.  During the classification, we choose the class of the closest cluster center.\n",
    "\n",
    "The data is preprocessed with `pygdf` DataFrame.  Initially, each row in the dataset is a single event.  Our preprocessing bins the event into frames and transpose the events in each frame into columns for a single record.  We then perform wavelet transformation to convert the time-domain into time-frequency domain.  These are done with the help of custom CUDA kernels written with `numba`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This automatically time every cell's execution\n",
    "!pip install ipython-autotime\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pygdf\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import holoviews as hv\n",
    "# import bokeh.palettes\n",
    "\n",
    "# hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    print('reading', path)\n",
    "    # Read using pandas \n",
    "    df = pd.read_csv(path, nrows=nrows, index_col='Index')\n",
    "    print('shape =', df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# if not os.path.exists('data/har/Activity recognition exp'):\n",
    "#     # Unzip data if not already there\n",
    "#     !unzip \"data/har/Activity recognition exp.zip\" -d data/har\n",
    "#     !ls data/har"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l \"Phones_accelerometer.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_df = read_data('Phones_accelerometer.csv')\n",
    "pg_df = read_data('Phones_gyroscope.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "cats = defaultdict(set)\n",
    "for df in [pa_df, pg_df]:\n",
    "    for col in ['Model', 'User', 'Device', 'gt']:\n",
    "        df[col] = df[col].astype('category')\n",
    "        cats[col] |= set(df[col].astype('category').cat.categories)\n",
    "        #cats[col].add('null')\n",
    "\n",
    "for col in ['Model', 'User', 'Device', 'gt']:\n",
    "    ordered_cats = sorted(cats[col])\n",
    "    print(col, ordered_cats)\n",
    "    for df in [pa_df, pg_df]:\n",
    "        df[col].cat.set_categories(ordered_cats)\n",
    "        #df[col] = df[col].fillna('null')\n",
    "        df[col] = df[col].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activities = tuple(df['gt'].cat.categories)\n",
    "# activities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to GPU DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_df = pygdf.DataFrame.from_pandas(pa_df)\n",
    "pg_df = pygdf.DataFrame.from_pandas(pg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_df.head().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_df.head().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More preprocessing on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude `gt == null`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#null_idx = pa_df['gt'].cat.categories.index('null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('before', len(pa_df), len(pg_df))\n",
    "#pa_df = pa_df.query('gt != @null_idx').reset_index()\n",
    "#pg_df = pg_df.query('gt != @null_idx').reset_index()\n",
    "pa_df = pa_df.query('gt != -1').reset_index()\n",
    "pg_df = pg_df.query('gt != -1').reset_index()\n",
    "print('after', len(pa_df), len(pg_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(sr):\n",
    "    maxval = max(abs(sr.max()), abs(sr.min()))\n",
    "    return sr / maxval\n",
    "\n",
    "for df in [pa_df, pg_df]:\n",
    "    for col in 'xyz':\n",
    "        df[col] = rescale(df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bin the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 1280000000\n",
    "subsample_size = 2**5\n",
    "print('subsample_size', subsample_size)\n",
    "dt2 = dt / subsample_size\n",
    "pa_df['resampled'] = (pa_df['Creation_Time'] // dt)\n",
    "pg_df['resampled'] = (pg_df['Creation_Time'] // dt)\n",
    "pa_df['resampled_inner'] = (pa_df['Creation_Time'] // dt2)\n",
    "pg_df['resampled_inner'] = (pg_df['Creation_Time'] // dt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Numba to JIT compile GPU kernels for transposing rows into columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda, float64\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def is_valid(offsets, valid):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < valid.size:\n",
    "        s = offsets[idx]\n",
    "        e = offsets[idx + 1]\n",
    "        valid[idx] = e - s == subsample_size\n",
    "\n",
    "@cuda.jit\n",
    "def expand_df(offsets, inp, out):\n",
    "    blkid = cuda.blockIdx.x\n",
    "    tid = cuda.threadIdx.x\n",
    "    row_start = offsets[blkid]\n",
    "    row_stop = offsets[blkid + 1]\n",
    "    if tid < out.shape[0]:\n",
    "        val = np.nan\n",
    "        if row_stop - row_start == out.shape[0]:\n",
    "            val = inp[row_start + tid]\n",
    "\n",
    "        out[tid, blkid] = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JIT custom GPU kernels for simple wavelet decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit(device=True)\n",
    "def inner_haar_wavelet(arr):\n",
    "    # assume pow-of-2 subsample_size\n",
    "    stride = subsample_size // 2\n",
    "    while stride:\n",
    "        i = cuda.threadIdx.x\n",
    "        if i < stride:\n",
    "            even = arr[2 * i]\n",
    "            odd = arr[2 * i + 1]\n",
    "            c0 = (even + odd) / 2\n",
    "            c1 = (even - odd) / 2\n",
    "        cuda.syncthreads()\n",
    "        if i < stride:\n",
    "            arr[i] = c0\n",
    "            arr[i + stride] = c1\n",
    "        cuda.syncthreads()\n",
    "        stride //= 2\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def haar_wavelet(arr):\n",
    "    sm_arr = cuda.shared.array((subsample_size,), dtype=float64)\n",
    "    blkid = cuda.blockIdx.x\n",
    "    i = cuda.threadIdx.x\n",
    "    sm_arr[i] = arr[i, blkid]\n",
    "    cuda.syncthreads()\n",
    "\n",
    "    inner_haar_wavelet(sm_arr)\n",
    "\n",
    "    arr[i, blkid] = sm_arr[i]\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GPU Groupby to resample and bin the time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(src_df):\n",
    "    # First groupby creates the frames. Each frame has 20 rows.\n",
    "    groupkeys = ['resampled', 'User', 'Model', 'Device', 'gt', 'resampled_inner']\n",
    "    df = src_df.loc[:, groupkeys + ['x', 'y', 'z']].groupby(groupkeys).mean()\n",
    "    # Second groupby transpose the rows in each frame into columns\n",
    "    grouped, segs = df.groupby(groupkeys[:-1]).as_df()\n",
    "    numgroups = len(segs)\n",
    "    d_offsets = cuda.to_device(np.hstack([segs.to_array(), len(grouped)]))\n",
    "\n",
    "    d_valid = cuda.device_array(numgroups, dtype=np.bool_)\n",
    "    d_xs = cuda.device_array((subsample_size, numgroups), dtype=np.float64)\n",
    "    d_ys = cuda.device_array((subsample_size, numgroups), dtype=np.float64)\n",
    "    d_zs = cuda.device_array((subsample_size, numgroups), dtype=np.float64)\n",
    "    \n",
    "    # Launch transposing CUDA kernel\n",
    "    # Each row of x, y, z becomes columns\n",
    "    expand_df[numgroups, subsample_size](d_offsets, grouped['x'].to_gpu_array(), d_xs)\n",
    "    expand_df[numgroups, subsample_size](d_offsets, grouped['y'].to_gpu_array(), d_ys)\n",
    "    expand_df[numgroups, subsample_size](d_offsets, grouped['z'].to_gpu_array(), d_zs)\n",
    "    is_valid.forall(d_offsets.size - 1)(d_offsets, d_valid)\n",
    "    \n",
    "    # Use wavelet to decompose the time-domain data into time-frequency data\n",
    "    haar_wavelet[numgroups, subsample_size](d_xs)\n",
    "    haar_wavelet[numgroups, subsample_size](d_ys)\n",
    "    haar_wavelet[numgroups, subsample_size](d_zs)\n",
    "    \n",
    "    # Creates the final resampled dataframe.\n",
    "    # Now, each row is a frame.\n",
    "    outdf = pygdf.DataFrame()\n",
    "    outdf['resampled'] = grouped['resampled'].take(d_offsets[:-1], ignore_index=True)\n",
    "    outdf['User'] = grouped['User'].take(d_offsets[:-1], ignore_index=True)\n",
    "    outdf['Device'] = grouped['Device'].take(d_offsets[:-1], ignore_index=True)\n",
    "    outdf['gt'] = grouped['gt'].take(d_offsets[:-1], ignore_index=True)\n",
    "    outdf['valid'] = d_valid\n",
    "    for colname, colvals in zip('xyz', [d_xs, d_ys, d_zs]):\n",
    "        for i in range(subsample_size):\n",
    "            fullcolname = '{}{}'.format(colname, i)\n",
    "            sr = pygdf.Series(colvals[i])\n",
    "            outdf[fullcolname] = rescale(sr)\n",
    "    \n",
    "    out = outdf.query(\"valid\")\n",
    "    out.drop_column(\"valid\")\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample the accelerometer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newpadf = expand(pa_df)\n",
    "print(len(newpadf))\n",
    "newpadf.head().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample the gyroscope data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newpgdf = expand(pg_df)\n",
    "print(len(newpgdf))\n",
    "newpgdf.head().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Inner-Join to combine the accelerometer and gyroscope data on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = newpadf.set_index('resampled').join(newpgdf.set_index('resampled'), how='inner',\n",
    "                                             lsuffix='_a', rsuffix='_g')\n",
    "joined.head().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the joined table.  Drop mismatching rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = joined.query(\"User_a == User_g and gt_a == gt_g and Device_a == Device_g\")\n",
    "print(len(filtered))\n",
    "filtered.head().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create features table for KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['gt_a'] + [k for k in filtered.columns if k[0] in 'xyz']\n",
    "features_df = filtered.loc[:, feature_columns]\n",
    "print(len(features_df))\n",
    "features_df.head().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize data and split into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df):\n",
    "    idx = np.arange(len(df))\n",
    "    np.random.shuffle(idx)\n",
    "    d_idx = cuda.to_device(idx)\n",
    "    outdf = pygdf.DataFrame()\n",
    "    for k in df.columns:\n",
    "        outdf[k] = df[k].take(d_idx, ignore_index=True) \n",
    "        \n",
    "    split_pt = int(len(outdf) * 0.8)\n",
    "    return outdf[:split_pt], outdf[split_pt:]\n",
    "    \n",
    "train_df, test_df = split_train_test(features_df)\n",
    "print('train size', len(train_df))\n",
    "print('test size', len(test_df))\n",
    "\n",
    "train_df.head().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [k for k in train_df.columns if k !='gt_a']\n",
    "feature_matrix = train_df.as_matrix(feature_columns)\n",
    "\n",
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Machine Learning with H2O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `h2o4gpu.KMeans` to cluster the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o4gpu\n",
    "\n",
    "kmeans = h2o4gpu.KMeans(n_clusters=80, n_gpus=1, max_iter=1000)\n",
    "kmeans.fit(feature_matrix)\n",
    "predicted = kmeans.predict(feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find meaning for each cluster\n",
    "\n",
    "For each cluster, the dominating class is adopted as the class for the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pygdf.DataFrame()\n",
    "\n",
    "gt_predicted = pygdf.Series(np.asarray(predicted, dtype=np.int32))\n",
    "pred_df['gt_predicted'] = gt_predicted.set_index(train_df.index)\n",
    "\n",
    "for k in features_df.columns:\n",
    "    pred_df[k] = train_df[k]\n",
    "\n",
    "pred_df.head().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode the \"activities\" (`gt_a` column).  The will serve as \"vote\" to determine the class for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = pred_df.copy()\n",
    "for name, col in zip(activities, \n",
    "                     pred_df['gt_a'].astype(np.int32).one_hot_encoding(cats=list(range(len(activities))))):\n",
    "    out_df[name] = col.set_index(pred_df.index)\n",
    "    \n",
    "out_df.head().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the \"vote\" for each cluster using groupby-mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [k for k in out_df.columns if k.startswith('gt') and k !='gt_a' or k in activities]\n",
    "finaldf = out_df.loc[:, cols].groupby('gt_predicted').mean()\n",
    "finaldf.head().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associate each cluster with the dominating activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "predict_dict = {}\n",
    "for row in range(len(finaldf)):\n",
    "    r = finaldf.gt_predicted[row]\n",
    "    best = max((finaldf[k][row], k) for k in activities)\n",
    "    print('cluster {}: {:.1f}% {}'.format(r,\n",
    "                                          best[0] * 100, \n",
    "                                          best[1]))\n",
    "    predict_dict[r] = \"{}.{}\".format(best[1], r)\n",
    "    \n",
    "predict_cats = [predict_dict.get(i, 'unknown.{}'.format(i)) for i in range(len(kmeans.cluster_centers_))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model with the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matrix = test_df.as_matrix(feature_columns)\n",
    "test_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = kmeans.predict(test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pygdf.DataFrame()\n",
    "\n",
    "eval_df['gt_predicted'] = pd.Categorical.from_codes(predict_test, categories=predict_cats)\n",
    "eval_df['gt_a'] = test_df['gt_a'].reset_index()\n",
    "\n",
    "eval_df.head(n=30).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = eval_df.to_pandas().apply(lambda sr: sr['gt_predicted'].split('.')[0] == sr['gt_a'], axis=1)\n",
    "num_correct = len(matches[matches == True])\n",
    "num_total = len(matches)\n",
    "print('{:.1f}% correctly predicted'.format(100 * num_correct / num_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use LDA to 2D-project our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "lda = LDA(n_components=2)\n",
    "lda_feat_mat = lda.fit(feature_matrix, train_df['gt_a'].to_array()).transform(feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = pygdf.DataFrame()\n",
    "cluster_df['x'] = np.require(lda_feat_mat[:, 0], requirements='C')\n",
    "cluster_df['y'] = np.require(lda_feat_mat[:, 1], requirements='C')\n",
    "cluster_df['activity'] = pred_df['gt_a']\n",
    "\n",
    "predicted_clusters = pd.Categorical.from_codes(pred_df['gt_predicted'].to_pandas(), categories=predict_cats)\n",
    "cluster_df['prediction'] = pd.Categorical([k.split('.')[0] for k in predicted_clusters])\n",
    "\n",
    "cluster_df.head().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Points [width=480, height=400]\n",
    "\n",
    "plot_df = cluster_df.to_pandas().sample(min(2000, len(cluster_df)))\n",
    "\n",
    "cmap = bokeh.palettes.Set1[8]\n",
    "\n",
    "pts_expected = {k : hv.Points(plot_df.loc[plot_df['activity'] == k])(style={'color': col}) \n",
    "                for k, col in zip(activities, cmap)}\n",
    "\n",
    "pts_predicted = {k : hv.Points(plot_df.loc[plot_df['prediction'] == k])(style={'color': col}) \n",
    "                for k, col in zip(activities, cmap)}\n",
    "\n",
    "left = hv.NdOverlay(pts_expected, kdims=['activity'])\n",
    "right = hv.NdOverlay(pts_predicted, kdims=['activity'])\n",
    "\n",
    "left.relabel('Expected') + right.relabel('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for best k in k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cluster_size(n_clusters, test_df):\n",
    "    # Clustering\n",
    "    kmeans = h2o4gpu.KMeans(n_clusters=n_clusters, n_gpus=1, max_iter=1000)\n",
    "    kmeans.fit(feature_matrix)\n",
    "    predicted = kmeans.predict(feature_matrix)\n",
    "\n",
    "    # Determine cluster meaning\n",
    "    pred_df = pygdf.DataFrame()\n",
    "\n",
    "    gt_predicted = pygdf.Series(np.asarray(predicted, dtype=np.int32))\n",
    "    pred_df['gt_predicted'] = gt_predicted.set_index(train_df.index)\n",
    "\n",
    "    for k in features_df.columns:\n",
    "        pred_df[k] = train_df[k]\n",
    "\n",
    "    out_df = pred_df.copy()\n",
    "    for name, col in zip(activities, \n",
    "                         pred_df['gt_a'].astype(np.int32).one_hot_encoding(cats=list(range(len(activities))))):\n",
    "        out_df[name] = col.set_index(pred_df.index)\n",
    "\n",
    "\n",
    "    cols = [k for k in out_df.columns if k.startswith('gt') and k !='gt_a' or k in activities]\n",
    "    finaldf = out_df.loc[:, cols].groupby('gt_predicted').mean()\n",
    "\n",
    "\n",
    "    predict_dict = {}\n",
    "    for row in range(len(finaldf)):\n",
    "        r = finaldf.gt_predicted[row]\n",
    "        best = max((finaldf[k][row], k) for k in activities)\n",
    "        predict_dict[r] = \"{}.{}\".format(best[1], r)\n",
    "\n",
    "    predict_cats = [predict_dict.get(i, 'unknown.{}'.format(i)) for i in range(len(kmeans.cluster_centers_))]\n",
    "\n",
    "    # Evaluate\n",
    "    test_matrix = test_df.as_matrix(feature_columns)\n",
    "\n",
    "    predict_test = kmeans.predict(test_matrix)\n",
    "\n",
    "    eval_df = pygdf.DataFrame()\n",
    "\n",
    "    eval_df['gt_predicted'] = pd.Categorical.from_codes(predict_test, categories=predict_cats)\n",
    "    eval_df['gt_a'] = test_df['gt_a'].reset_index()\n",
    "\n",
    "    eval_df.head(n=30).to_pandas()\n",
    "\n",
    "    matches = eval_df.to_pandas().apply(lambda sr: sr['gt_predicted'].split('.')[0] == sr['gt_a'], axis=1)\n",
    "    num_correct = len(matches[matches == True])\n",
    "    num_total = len(matches)\n",
    "    correctness = num_correct / num_total\n",
    "    print('n_clusters={} | {:.1f}% correctly predicted'.format(n_clusters, 100 * correctness))\n",
    "    return correctness\n",
    "    \n",
    "\n",
    "print(\"Scan testing data\")\n",
    "cc_test = [(k, eval_cluster_size(k, test_df=test_df)) for k in range(10, 500, 50)]\n",
    "print(\"Scan training data\")\n",
    "cc_train = [(k, eval_cluster_size(k, test_df=train_df)) for k in range(10, 500, 50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot accuracy vs num of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Curve [width=500, height=400, show_grid=True]\n",
    "\n",
    "\n",
    "def cc_make_df(cc):\n",
    "    xs, ys = zip(*cc)\n",
    "    df = pd.DataFrame()\n",
    "    df['num_cluster'] = xs\n",
    "    df['accuracy %'] = np.asarray(ys) * 100\n",
    "    return df\n",
    "\n",
    "\n",
    "df_test = cc_make_df(cc_test)\n",
    "df_train = cc_make_df(cc_train)\n",
    "\n",
    "title = 'Accuracy vs Number of Clusters'\n",
    "(hv.Scatter(df_test) * hv.Curve(df_test, label='test') * hv.Scatter(df_train) * hv.Curve(df_train, label='train')).relabel(title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
